Bulusu, 2020, https://arxiv.org/pdf/2003.06979.pdf, "ANOMALOUS INSTANCE DETECTION IN DEEP LEARNING: A SURVEY",
Meinke, 2020, https://arxiv.org/abs/1909.12180, "TOWARDS NEURAL NETWORKS THAT PROVABLY KNOWWHEN THEY DON’T KNOW"
Serrà, 2020, https://arxiv.org/pdf/1909.11480.pdfn "Input complexity and out-of-distribution detection with likelihood-based generative models"
Hendrycks, 2020, https://arxiv.org/pdf/1912.02781.pdf, "AUGMIX:  A SIMPLE DATA PROCESSING METHOD  TO IMPROVE ROBUSTNESS AND UNCERTAINTY"
Liu, 2019, http://openaccess.thecvf.com/content_CVPR_2019/html/Liu_Large-Scale_Long-Tailed_Recognition_in_an_Open_World_CVPR_2019_paper.html, "Large-Scale Long-Tailed Recognition in an Open World"
Hendrycks, 2019, http://papers.nips.cc/paper/9697-using-self-supervised-learning-can-improve-model-robustness-and-uncertainty, "Using Self-Supervised Learning Can Improve Model Robustness and Uncertainty"
Song, 2019, https://arxiv.org/pdf/1910.09115.pdf, "Unsupervised Out-of-Distribution Detection with Batch Normalization"
Tagasovska, 2019, http://papers.nips.cc/paper/8870-single-model-uncertainties-for-deep-learning, "Single-Model Uncertainties for Deep Learning"
Hein, 2019, http://openaccess.thecvf.com/content_CVPR_2019/html/Hein_Why_ReLU_Networks_Yield_High-Confidence_Predictions_Far_Away_From_the_CVPR_2019_paper.html, "Why ReLU Networks Yield High-Confidence Predictions Far Away From the Training Data and How to Mitigate the Problem",
Ahuja, 2019, https://arxiv.org/pdf/1909.11786.pdf, "Probabilistic Modeling of Deep Features for Out-of-Distribution and Adversarial Detection"
Hendrycks, 2019, https://arxiv.org/pdf/1901.09960.pdf, "Using Pre-Training Can Improve Model Robustness and Uncertainty"
Ren, 2019, https://arxiv.org/pdf/1906.02845.pdf, "Likelihood Ratios for Out-of-Distribution Detection"
Nalisnick, 2019, https://arxiv.org/pdf/1810.09136.pdf, "DO DEEP GENERATIVE MODELS KNOW WHAT THEY DON’T KNOW?"
Mu, 2019, https://arxiv.org/pdf/1906.02337.pdf, "MNIST-C A Robustness Benchmark for Computer Vision"
Papadopoulos,2019,https://arxiv.org/pdf/1906.03509.pdf,"OUTLIEREXPOSURE WITHCONFIDENCECONTROLFOROUT-OF-DISTRIBUTIONDETECTION"
Zheng, 2019, https://arxiv.org/pdf/1909.03862.pdf, "Out-of-domain Detection for Natural LanguageUnderstanding in Dialog Systems"
Hendrycks, 2019, https://arxiv.org/pdf/1903.12261.pdf, "Benchmarking Neural Network Robustness To Common Corruptions and Perturbations"
Hendrycks, 2019, https://arxiv.org/pdf/1812.04606.pdf, "Deep Anomaly Detection With Outlier-Exposure"
Snoek, 2019, http://papers.nips.cc/paper/9547-can-you-trust-your-models-uncertainty-evaluating-predictive-uncertainty-under-dataset-shift, "Can you trust your model's uncertainty? Evaluating predictive uncertainty under dataset shift"
Shafaei, 2018, https://arxiv.org/abs/1809.04729, "A Less Biased Evaluation of Out-of-distribution Sample Detectors"
Liang, 2018, https://arxiv.org/pdf/1706.02690.pdf, "ENHANCING THE RELIABILITY  OFOUT-OF-DISTRIBUTION IMAGE DETECTION IN NEURAL NETWORKS"
DeVries, 2018, https://arxiv.org/pdf/1802.04865.pdf, "Learning Confidence for Out-of-Distribution Detection in Neural Networks"
Lee, 2018, https://arxiv.org/pdf/1711.09325.pdf, "Training Confidence-Calibrated Classifiers for Detecting Out-of-Distribution Samples"
Dietterich, 2018, https://arxiv.org/pdf/1809.01605.pdf, "Anomaly Detection in the Presence of Missing Values"
Lee, 2017, https://papers.nips.cc/paper/7947-a-simple-unified-framework-for-detecting-out-of-distribution-samples-and-adversarial-attacks.pdf, "A Simple Unified Framework for DetectingOut-of-Distribution Samples and Adversarial Attacks"
Hendrycks,2017,https://arxiv.org/pdf/1610.02136.pdf,"A  BASELINE   FORDETECTINGMISCLASSIFIED   ANDOUT-OF-DISTRIBUTIONEXAMPLESINNEURALNETWORKS"
Lakshminarayanan, 2017, http://papers.nips.cc/paper/7219-simple-and-scalable-predictive-uncertainty-estimation-using-deep-ensembles, "Simple and Scalable Predictive Uncertainty Estimation using Deep Ensembles"
Guo, 2017, https://dl.acm.org/doi/10.5555/3305381.3305518, "On calibration of modern neural networks"